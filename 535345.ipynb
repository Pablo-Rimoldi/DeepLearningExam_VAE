{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Solution: VAE for Image Reconstruction and Generation\n",
    "\n",
    "**Student:** Rimoldi\n",
    "\n",
    "**Exam Session:** July 15, 2025\n",
    "\n",
    "This notebook implements the proposed solution for the exam. The goal is to design a deep neural network capable of encoding images into a probabilistic latent space, then reconstructing or generating new images, closely following the structure of the provided example code and the numerical order of the exam sheet.\n",
    "\n",
    "Notes :\n",
    "- This notebook takes around 6 minutes to run.\n",
    "- I underlined the differences between the exam in the following code like this: <font color=\"red\">**CHANGE**</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (4.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, ops, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces the dataset loading process, utilizing the requests library to download the necessary data from the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/PaRi29/DeepLearningExam_VAE/main/assets/input_data.pkl...\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/PaRi29/DeepLearningExam_VAE/main/assets/input_data.pkl\"\n",
    "print(f\"Downloading data from {url}...\")\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "with open(\"input_data.pkl\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: MODEL\n",
    "\n",
    "**Model Choice:** Variational Autoencoder (VAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2: INPUT AND PREPROCESSING\n",
    "\n",
    "Normalization: Since we are working with grayscale images, pixel values are normalized from the [0, 255] range to the [0, 1] range by dividing each value by 255.0. This process helps stabilize and speed up model training.\n",
    "\n",
    "Input Format (Shape): An additional channel dimension is added to the images (since they are grayscale, there is only one channel). The resulting input shape is (batch_size, 28, 28, 1), which is required by convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Training data shape: (5600, 28, 28, 1)\n",
      "Validation data shape: (1400, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"input_data.pkl\", \"rb\") as f:\n",
    "        dd = pickle.load(f)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    data = dd['data']\n",
    "    labels = dd['labels']\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "def preprocess_images(images):\n",
    "    images = images.astype(\"float32\") / 255.0\n",
    "    images = np.expand_dims(images, axis=-1)\n",
    "    return images\n",
    "\n",
    "data = preprocess_images(data)\n",
    "\n",
    "# Manually split data and labels into training and validation sets\n",
    "val_fraction = 0.2\n",
    "num_samples = data.shape[0]\n",
    "num_val = int(num_samples * val_fraction)\n",
    "\n",
    "# Shuffle the data before splitting\n",
    "indices = np.arange(num_samples)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_val = data[:num_val]\n",
    "x_train = data[num_val:]\n",
    "\n",
    "INPUT_SHAPE = x_train.shape[1:]\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Validation data shape: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: MODEL CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model Composition\n",
    "The VAE consists of an Encoder, a Decoder, and a specialized `Sampling` layer. \n",
    "**Conceptual Outline:** `Input Image` -> **[Encoder]** -> `Latent Distribution` -> **[Sampling]** -> `Latent Vector` -> **[Decoder]** -> `Reconstructed Image`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Image Projection (Encoder)\n",
    "The Encoder is a CNN that maps an image to a probabilistic latent space. It reduces the image's dimensionality through convolutional and pooling layers, finally outputting the mean (`z_mean`) and log-variance (`z_log_var`) of the learned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Image Reconstruction (Decoder)\n",
    "The Decoder is a deconvolutional network, symmetric to the encoder. It takes a latent vector `z` (sampled from the distribution defined by `z_mean` and `z_log_var`), upsamples it through transposed convolutions (or upsampling layers), and reconstructs the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Generation of New Images\n",
    "To generate new images, we discard the encoder and use only the trained decoder. A random vector `z` is sampled from a standard normal distribution N(0,1) and passed to the decoder, which then generates a new, coherent image that was not in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Hyperparameters\n",
    "Key hyperparameters to tune include `learning_rate`, `batch_size`, `latent_dim` (the size of the latent space), and `kl_reg` (the weight of the KL divergence loss). A random search is performed to find a good combination of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z (reparameterization trick).\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim))\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def vae_enc(input_shape, latent_dim, filter_base=32, dense_units=16):\n",
    "    \"\"\"Encoder model (Projection), configurable via hyperparameters.\"\"\"\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(filter_base, 3, activation=\"relu\", padding=\"same\")(encoder_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2, padding=\"same\")(x)  # 28x28 -> 14x14\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.Conv2D(filter_base * 2, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2, padding=\"same\")(x)  # 14x14 -> 7x7\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    return encoder\n",
    "\n",
    "def vae_dec(latent_dim, filter_base=32, dense_units=16):\n",
    "    \"\"\"Decoder model (Reconstruction), configurable via hyperparameters.\"\"\"\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    \n",
    "    # Initial part to prepare for reconstruction\n",
    "    x = layers.Dense(7 * 7 * (filter_base * 2), activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((7, 7, filter_base * 2))(x)\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.UpSampling2D(2)(x)  # 7x7 -> 14x14\n",
    "    x = layers.Conv2D(filter_base * 2, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.UpSampling2D(2)(x)  # 14x14 -> 28x28\n",
    "    x = layers.Conv2D(filter_base, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    return decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid and search function for hyperparameters (Point 3e)\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 1e-3],\n",
    "    'batch_size':    [64, 128],\n",
    "    'latent_dim':    [2, 8],\n",
    "    'kl_reg':        [0.5, 1.0, 2.0],\n",
    "    'optimizer':     ['adam', 'rmsprop'],\n",
    "    'filter_base':   [16, 32],\n",
    "    'dense_units':   [16, 32],\n",
    "    'patience':      [2, 3, 5],\n",
    "    'min_delta':     [0.0, 1e-4, 1e-3],\n",
    "    'restore_best_weights': [True, False],\n",
    "    'monitor':       ['val_loss', 'val_reconstruction_loss']\n",
    "}\n",
    "num_samples = 5 # Number of random combinations to test\n",
    "\n",
    "\n",
    "def random_search_vae(VAE_MODEL, INPUT_SHAPE, param_grid, x_train, x_val, samples=num_samples):\n",
    "    \"\"\"\n",
    "    Performs a random search over the defined parameter grid.\n",
    "    \n",
    "    Args:\n",
    "        VAE_MODEL: The VAE model class (which must be defined).\n",
    "        INPUT_SHAPE: The shape of the input images.\n",
    "        param_grid: A dictionary of hyperparameters to search over.\n",
    "        x_train: Training data.\n",
    "        x_val: Validation data.\n",
    "        samples: The number of random combinations to try.\n",
    "    \n",
    "    Returns:\n",
    "        The dictionary containing the best hyperparameter configuration found.\n",
    "    \"\"\"\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    sampled_combos = random.sample(combos, min(samples, len(combos)))\n",
    "    configs = [dict(zip(param_grid.keys(), c)) for c in sampled_combos]\n",
    "    best_val_loss = np.inf\n",
    "    best_cfg = None\n",
    "\n",
    "    for idx, cfg in enumerate(configs):\n",
    "        print(f\"\\n=== Training config {idx+1}/{len(configs)} ===\")\n",
    "        print(\"Config:\", cfg)\n",
    "        \n",
    "        # Build model with current config\n",
    "        encoder = vae_enc(INPUT_SHAPE, cfg['latent_dim'], cfg['filter_base'], cfg['dense_units'])\n",
    "        decoder = vae_dec(cfg['latent_dim'], cfg['filter_base'], cfg['dense_units'])\n",
    "        vae_model = VAE_MODEL(encoder, decoder, reg=cfg['kl_reg'])\n",
    "        \n",
    "        # Select and configure optimizer\n",
    "        if cfg['optimizer'] == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=cfg['learning_rate'])\n",
    "        else: # rmsprop\n",
    "            optimizer = keras.optimizers.RMSprop(learning_rate=cfg['learning_rate'])\n",
    "            \n",
    "        vae_model.compile(optimizer=optimizer)\n",
    "        \n",
    "        # Define Early Stopping callback with all relevant hyperparameters\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor=cfg['monitor'],\n",
    "            patience=cfg['patience'],\n",
    "            min_delta=cfg['min_delta'],\n",
    "            verbose=1,\n",
    "            restore_best_weights=cfg['restore_best_weights']\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        history = vae_model.fit(\n",
    "            x_train, x_train,\n",
    "            epochs=50, # Set a high number; Early Stopping will terminate training\n",
    "            batch_size=cfg['batch_size'],\n",
    "            validation_data=(x_val, x_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0 # Suppress epoch-by-epoch output for cleaner search\n",
    "        )\n",
    "        \n",
    "        # Get the best validation loss from this run (thanks to restore_best_weights)\n",
    "        # Use the same metric as monitored\n",
    "        monitor_metric = cfg['monitor']\n",
    "        if monitor_metric in history.history:\n",
    "            final_val_loss = min(history.history[monitor_metric])\n",
    "        else:\n",
    "            # fallback to val_loss if custom metric not found\n",
    "            final_val_loss = min(history.history['val_loss'])\n",
    "        print(f\"Best {monitor_metric} for this config: {final_val_loss:.4f}\")\n",
    "        \n",
    "        if final_val_loss < best_val_loss:\n",
    "            best_val_loss = final_val_loss\n",
    "            best_cfg = cfg\n",
    "            print(f\"*** New best model found! ***\")\n",
    "\n",
    "    print(\"\\n=== Best Results Summary ===\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"Best configuration:\", best_cfg)\n",
    "    return best_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: OUTPUT\n",
    "\n",
    "The output layer of the decoder is a `Conv2D` layer designed as follows:\n",
    "- **Filters:** `1`. This is because the output images are grayscale and have only one channel.\n",
    "- **Activation Function:** `sigmoid`. The input images were normalized to the range [0, 1]. The sigmoid function outputs values in this same [0, 1] range, making it the natural choice for the final layer. This ensures the reconstructed pixel values are on the same scale as the input, which is essential for calculating the reconstruction loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LOSS\n",
    "\n",
    "CHANGE with respect to exam: In the exam, I **mistakenly used Mean Absolute Error (MAE)** as the reconstruction loss.  \n",
    "While MAE is sometimes used in **standard autoencoders** due to its robustness to outliers, it is **not the canonical choice for Variational Autoencoders (VAEs)**.\n",
    "\n",
    "The loss function for a **VAE** consists of two main components, which reflect its **probabilistic generative nature**:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Reconstruction Loss**  \n",
    "For VAEs, the reconstruction term is derived from the assumption that pixels are generated from a Gaussian distribution. Under this assumption, the appropriate reconstruction loss is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{recon}} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{x}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is the original pixel value,\n",
    "- $\\hat{x}_i$ is the reconstructed pixel value,\n",
    "- $N$ is the total number of pixels.\n",
    "\n",
    "\n",
    "**2. Kullback-Leibler (KL) Divergence**  \n",
    "This term ensures that the latent space distribution learned by the encoder remains close to a prior distribution (usually a standard normal $\\mathcal{N}(0, I)$):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum_{j=1}^{d} \\left(1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_j$ and $\\sigma_j$ are the predicted mean and standard deviation for latent dimension $j$,\n",
    "- $d$ is the dimensionality of the latent space.\n",
    "\n",
    "This term is **crucial** for enabling the model to generalize and generate new data by sampling from the latent space.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Total Loss Function**\n",
    "\n",
    "The final VAE loss combines the two components:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{recon}} + \\beta \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\beta$ is a hyperparameter that balances reconstruction accuracy with latent space regularization.  \n",
    "  In the standard VAE, $\\beta = 1$, but it can be tuned (e.g., in $\\beta$-VAEs) to control disentanglement.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "My initial response mistakenly applied a loss formulation more suited to standard autoencoders.  \n",
    "A correct VAE implementation must:\n",
    "- Use **MSE** for reconstruction (derived from Gaussian likelihood),\n",
    "- Include the **KL divergence** for latent regularization,\n",
    "- Combine both in a total loss that supports both reconstruction and generative capabilities.\n",
    "\n",
    "This correction reflects a deeper understanding of **why** these loss terms are used in VAEs and how they relate to the modelâ€™s probabilistic foundations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, reg=1.0, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.reg = reg\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = ops.mean(\n",
    "                ops.sum(keras.losses.mean_squared_error(data, reconstruction), axis=(1, 2))\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + self.reg * kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        z_mean, z_log_var, z = self.encoder(data, training=False)\n",
    "        reconstruction = self.decoder(z, training=False)\n",
    "        reconstruction_loss = ops.mean(\n",
    "            ops.sum(keras.losses.mean_squared_error(data, reconstruction), axis=(1, 2))\n",
    "        )\n",
    "        kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "        kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "        total_loss = reconstruction_loss + self.reg * kl_loss\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        _, _, z = self.encoder(inputs)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: MODEL EVALUATION\n",
    "\n",
    "The model is evaluated through a comprehensive process that includes hyperparameter tuning, final training, and both quantitative and qualitative assessment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search Execution\n",
    "First, we execute the random search defined in Point 3e to find the best hyperparameters using a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a validation set for hyperparameter tuning\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m x_train, x_val = train_test_split(\u001b[43mx_train_full\u001b[49m, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Execute the search\u001b[39;00m\n\u001b[32m      5\u001b[39m best_config = random_search_vae(param_grid, x_train, x_val, VAE_MODEL=VAE)\n",
      "\u001b[31mNameError\u001b[39m: name 'x_train_full' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a validation set for hyperparameter tuning\n",
    "x_train, x_val = train_test_split(x_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Execute the search\n",
    "best_config = random_search_vae(param_grid, x_train, x_val, VAE_MODEL=VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Training\n",
    "\n",
    "Using the best hyperparameters found, we instantiate and train a final model on the entire training dataset for a larger number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta i migliori iperparametri trovati o usa i default se la ricerca non ha avuto successo\n",
    "if best_config is None:\n",
    "    print(\"Random search fallita o saltata. Uso valori di default.\")\n",
    "    best_config = {\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 128,\n",
    "        'latent_dim': 2,\n",
    "        'kl_reg': 1.0,\n",
    "        'filter_base': 16,\n",
    "        'dense_units': 16,\n",
    "        'optimizer': 'adam',\n",
    "        'patience': 3,\n",
    "        'min_delta': 1e-4,\n",
    "        'restore_best_weights': True,\n",
    "        'monitor': 'val_loss'\n",
    "    }\n",
    "\n",
    "LATENT_DIM = best_config.get('latent_dim', 2)\n",
    "BATCH_SIZE = best_config.get('batch_size', 128)\n",
    "LEARNING_RATE = best_config.get('learning_rate', 1e-3)\n",
    "KL_REG = best_config.get('kl_reg', 1.0)\n",
    "FILTER_BASE = best_config.get('filter_base', 16)\n",
    "DENSE_UNITS = best_config.get('dense_units', 16)\n",
    "OPTIMIZER = best_config.get('optimizer', 'adam')\n",
    "EPOCHS = 20\n",
    "\n",
    "print(\"\\n--- Training Final Model with Best Hyperparameters ---\")\n",
    "print(f\"Config: {best_config}\")\n",
    "\n",
    "# Crea encoder e decoder con la struttura aggiornata\n",
    "final_encoder = vae_enc(INPUT_SHAPE, LATENT_DIM, filter_base=FILTER_BASE, dense_units=DENSE_UNITS)\n",
    "final_decoder = vae_dec(LATENT_DIM, filter_base=FILTER_BASE, dense_units=DENSE_UNITS)\n",
    "final_vae = VAE(final_encoder, final_decoder, reg=KL_REG)\n",
    "\n",
    "if OPTIMIZER == 'adam':\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "elif OPTIMIZER == 'rmsprop':\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=LEARNING_RATE)\n",
    "else:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "final_vae.compile(optimizer=optimizer)\n",
    "\n",
    "final_encoder.summary()\n",
    "final_decoder.summary()\n",
    "\n",
    "# EarlyStopping callback se richiesto\n",
    "callbacks = []\n",
    "if 'patience' in best_config and 'monitor' in best_config:\n",
    "    callbacks.append(\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=best_config['monitor'],\n",
    "            patience=best_config['patience'],\n",
    "            min_delta=best_config.get('min_delta', 0.0),\n",
    "            restore_best_weights=best_config.get('restore_best_weights', True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "history = final_vae.fit(\n",
    "    x_train_full, x_train_full,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(x_test, x_test),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Assessment\n",
    "The reconstruction capabilities are evaluated both quantitatively and qualitatively.\n",
    "\n",
    "1.  **Quantitative Evaluation:** We use the trained model to reconstruct the test set images and calculate the **Mean Squared Error (MSE)** and **Mean Absolute Error (MAE)** between the original and reconstructed images. Lower values indicate better performance.\n",
    "2.  **Qualitative Evaluation:** We visually inspect the results by plotting original test images against their reconstructions. This helps assess the fidelity and sharpness of the output. We also visualize the generative latent space (if 2D) to confirm that the model has learned a smooth manifold for generating new, coherent images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluation of Reconstruction\n",
    "reconstructed_images = final_vae.predict(x_test)\n",
    "test_mse = mean_squared_error(x_test.flatten(), reconstructed_images.flatten())\n",
    "\n",
    "print(f\"\\nTest Set Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse:.4f}\")\n",
    "\n",
    "# Qualitative Evaluation\n",
    "def plot_reconstructions(original, reconstructed, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.suptitle(\"Reconstruction Evaluation: Original vs Reconstructed\")\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(\"Original\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(\"Reconstructed\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "def plot_latent_space(vae, n=15, figsize=15):\n",
    "    digit_size = 28\n",
    "    scale = 1.5\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vae.decoder.predict(z_sample, verbose=0)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size] = digit\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    plt.title(\"Generative Latent Space Exploration\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- Qualitative Evaluation ---\")\n",
    "plot_reconstructions(x_test, reconstructed_images)\n",
    "\n",
    "# The latent space plot is only meaningful for 2D latent spaces\n",
    "if LATENT_DIM == 2:\n",
    "    print(\"\\n--- Latent Space Visualization (for LATENT_DIM=2) ---\")\n",
    "    plot_latent_space(final_vae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
